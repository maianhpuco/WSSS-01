
CondaError: Run 'conda init' before 'conda deactivate'

/project/hnguyen2/mvu9/conda_envs/tpro/lib/python3.9/site-packages/wandb/apis/public.py:3078: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import parse_version
/project/hnguyen2/mvu9/conda_envs/tpro/lib/python3.9/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.24). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
2025-10-14 04:41:21,042 - main_train_tpro.py - INFO: 
args: namespace(config='/project/hnguyen2/mvu9/folder_04_ma/WSSS-01/src/external_modify/TPRO/bcss/classification/config.generated.yaml', local_rank=0, backend='nccl', wandb_log=False)
2025-10-14 04:41:21,044 - main_train_tpro.py - INFO: 
configs: {'model': {'backbone': {'config': 'mit_b1', 'stride': [4, 2, 2, 1]}, 'label_feature_path': 'medclip/bcss_label_fea', 'knowledge_feature_path': 'clinical_bert/bcss_knowledge_fea', 'n_ratio': 0.5}, 'dataset': {'name': 'bcss', 'train_root': '/project/hnguyen2/mvu9/datasets/weakly_sup_segmentation_datasets/BCSS-WSSS/training', 'val_root': '/project/hnguyen2/mvu9/folder_04_ma/WSSS-01/src/externals/TPRO/data/BCSS-WSSS', 'cls_num_classes': 4, 'input_size': [224, 224]}, 'work_dir': {'ckpt_dir': '/project/hnguyen2/mvu9/wsss_workingdir/ckpt_dir/classification/2025-10-14-04-41', 'pred_dir': '/project/hnguyen2/mvu9/wsss_workingdir/predictions/classification', 'train_log_dir': '/project/hnguyen2/mvu9/wsss_workingdir/train_log/classification', 'dir': '/project/hnguyen2/mvu9/folder_04_ma/WSSS-01/src/external_modify/TPRO/bcss/classification'}, 'train': {'samples_per_gpu': 10, 'epoch': 8, 'pretrained': True, 'l1': 0.0, 'l2': 0.1, 'l3': 1.0, 'l4': 1.0}, 'optimizer': {'type': 'AdamW', 'learning_rate': 0.0001, 'betas': [0.9, 0.999], 'weight_decay': 0.001}, 'scheduler': {'warmup_iter': 0, 'warmup_ratio': 1e-06, 'power': 1.0}}
2025-10-14 04:41:21,047 - distributed_c10d.py - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2025-10-14 04:41:21,175 - train_cls.py - INFO: use 23422 images for training, 3418 images for validation
2025-10-14 04:41:21,175 - train_cls.py - INFO: use 1 GPUs: [0]
/project/hnguyen2/mvu9/conda_envs/tpro/lib/python3.9/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
2025-10-14 04:41:21,744 - train_cls.py - INFO: 
Network config: 
ClsNetwork(
  (encoder): mit_b1(
    (patch_embed1): OverlapPatchEmbed(
      (proj): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))
      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed2): OverlapPatchEmbed(
      (proj): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed3): OverlapPatchEmbed(
      (proj): Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
    )
    (patch_embed4): OverlapPatchEmbed(
      (proj): Conv2d(320, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
    (block1): ModuleList(
      (0): Block(
        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=64, out_features=64, bias=True)
          (kv): Linear(in_features=64, out_features=128, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(64, 64, kernel_size=(8, 8), stride=(8, 8))
          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath(drop_prob=0.014)
        (norm2): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
          )
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)
    (block2): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath(drop_prob=0.029)
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=128, out_features=128, bias=True)
          (kv): Linear(in_features=128, out_features=256, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(128, 128, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath(drop_prob=0.043)
        (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          )
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm2): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
    (block3): ModuleList(
      (0): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath(drop_prob=0.057)
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=320, out_features=320, bias=True)
          (kv): Linear(in_features=320, out_features=640, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=320, out_features=320, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
          (sr): Conv2d(320, 320, kernel_size=(2, 2), stride=(2, 2))
          (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
        )
        (drop_path): DropPath(drop_prob=0.071)
        (norm2): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=320, out_features=1280, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1280)
          )
          (act): GELU()
          (fc2): Linear(in_features=1280, out_features=320, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm3): LayerNorm((320,), eps=1e-06, elementwise_affine=True)
    (block4): ModuleList(
      (0): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.086)
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (q): Linear(in_features=512, out_features=512, bias=True)
          (kv): Linear(in_features=512, out_features=1024, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (dwconv): DWConv(
            (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)
          )
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm4): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  )
  (l_fc1): AdaptiveLayer(
    (fc1): Linear(in_features=512, out_features=256, bias=True)
    (fc2): Linear(in_features=256, out_features=64, bias=True)
    (relu): ReLU()
  )
  (l_fc2): AdaptiveLayer(
    (fc1): Linear(in_features=512, out_features=256, bias=True)
    (fc2): Linear(in_features=256, out_features=128, bias=True)
    (relu): ReLU()
  )
  (l_fc3): AdaptiveLayer(
    (fc1): Linear(in_features=512, out_features=256, bias=True)
    (fc2): Linear(in_features=256, out_features=320, bias=True)
    (relu): ReLU()
  )
  (l_fc4): AdaptiveLayer(
    (fc1): Linear(in_features=512, out_features=256, bias=True)
    (fc2): Linear(in_features=256, out_features=512, bias=True)
    (relu): ReLU()
  )
  (k_fc4): AdaptiveLayer(
    (fc1): Linear(in_features=768, out_features=384, bias=True)
    (fc2): Linear(in_features=384, out_features=512, bias=True)
    (relu): ReLU()
  )
  (ka4): ModuleList(
    (0): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=512, out_features=512, bias=True)
        (kv): Linear(in_features=512, out_features=1024, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.100)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (q): Linear(in_features=512, out_features=512, bias=True)
        (kv): Linear(in_features=512, out_features=1024, bias=True)
        (attn_drop): Dropout(p=0, inplace=False)
        (proj): Linear(in_features=512, out_features=512, bias=True)
        (proj_drop): Dropout(p=0, inplace=False)
      )
      (drop_path): DropPath(drop_prob=0.100)
      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0, inplace=False)
      )
    )
  )
)
2025-10-14 04:41:24,094 - train_cls.py - INFO: 
Optimizer: 
PolyWarmupAdamW (
Parameter Group 0
    amsgrad: False
    betas: [0.9, 0.999]
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.001

Parameter Group 1
    amsgrad: False
    betas: [0.9, 0.999]
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0.0
)
[W reducer.cpp:1050] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters. This flag results in an extra traversal of the autograd graph every iteration, which can adversely affect performance. If your model indeed never has any unused parameters, consider turning this flag off. Note that this warning may be a false positive your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:362] Warning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [2048, 1, 3, 3], strides() = [9, 1, 3, 1]
bucket_view.sizes() = [2048, 1, 3, 3], strides() = [9, 9, 3, 1] (function operator())
2025-10-14 04:41:32,504 - train_cls.py - INFO: Iter: 100 / 18744; Elasped: 0:00:11; ETA: 0:34:10; LR: 9.947e-05; cls_loss: 0.8604; all_acc4: 40.00; avg_acc4: 82.50
2025-10-14 04:41:39,651 - train_cls.py - INFO: Iter: 200 / 18744; Elasped: 0:00:18; ETA: 0:27:48; LR: 9.894e-05; cls_loss: 0.9049; all_acc4: 50.00; avg_acc4: 80.00
2025-10-14 04:41:46,799 - train_cls.py - INFO: Iter: 300 / 18744; Elasped: 0:00:25; ETA: 0:25:37; LR: 9.840e-05; cls_loss: 0.4608; all_acc4: 60.00; avg_acc4: 90.00
2025-10-14 04:41:53,940 - train_cls.py - INFO: Iter: 400 / 18744; Elasped: 0:00:32; ETA: 0:24:27; LR: 9.787e-05; cls_loss: 0.9202; all_acc4: 40.00; avg_acc4: 82.50
2025-10-14 04:42:01,085 - train_cls.py - INFO: Iter: 500 / 18744; Elasped: 0:00:40; ETA: 0:24:19; LR: 9.734e-05; cls_loss: 0.6521; all_acc4: 50.00; avg_acc4: 82.50
2025-10-14 04:42:08,221 - train_cls.py - INFO: Iter: 600 / 18744; Elasped: 0:00:47; ETA: 0:23:41; LR: 9.680e-05; cls_loss: 1.0113; all_acc4: 30.00; avg_acc4: 77.50
2025-10-14 04:42:15,359 - train_cls.py - INFO: Iter: 700 / 18744; Elasped: 0:00:54; ETA: 0:23:11; LR: 9.627e-05; cls_loss: 0.6215; all_acc4: 70.00; avg_acc4: 87.50
2025-10-14 04:42:22,498 - train_cls.py - INFO: Iter: 800 / 18744; Elasped: 0:01:01; ETA: 0:22:48; LR: 9.574e-05; cls_loss: 0.5226; all_acc4: 60.00; avg_acc4: 87.50
2025-10-14 04:42:29,636 - train_cls.py - INFO: Iter: 900 / 18744; Elasped: 0:01:08; ETA: 0:22:28; LR: 9.520e-05; cls_loss: 1.1643; all_acc4: 30.00; avg_acc4: 80.00
2025-10-14 04:42:36,775 - train_cls.py - INFO: Iter: 1000 / 18744; Elasped: 0:01:15; ETA: 0:22:10; LR: 9.467e-05; cls_loss: 0.7467; all_acc4: 60.00; avg_acc4: 80.00
2025-10-14 04:42:43,918 - train_cls.py - INFO: Iter: 1100 / 18744; Elasped: 0:01:22; ETA: 0:21:55; LR: 9.414e-05; cls_loss: 0.8265; all_acc4: 70.00; avg_acc4: 90.00
2025-10-14 04:42:51,060 - train_cls.py - INFO: Iter: 1200 / 18744; Elasped: 0:01:30; ETA: 0:21:55; LR: 9.360e-05; cls_loss: 0.3877; all_acc4: 80.00; avg_acc4: 95.00
2025-10-14 04:42:58,214 - train_cls.py - INFO: Iter: 1300 / 18744; Elasped: 0:01:37; ETA: 0:21:41; LR: 9.307e-05; cls_loss: 0.3292; all_acc4: 80.00; avg_acc4: 95.00
2025-10-14 04:43:05,360 - train_cls.py - INFO: Iter: 1400 / 18744; Elasped: 0:01:44; ETA: 0:21:28; LR: 9.254e-05; cls_loss: 0.3098; all_acc4: 70.00; avg_acc4: 92.50
2025-10-14 04:43:12,502 - train_cls.py - INFO: Iter: 1500 / 18744; Elasped: 0:01:51; ETA: 0:21:16; LR: 9.200e-05; cls_loss: 0.3485; all_acc4: 80.00; avg_acc4: 95.00
2025-10-14 04:43:19,640 - train_cls.py - INFO: Iter: 1600 / 18744; Elasped: 0:01:58; ETA: 0:21:04; LR: 9.147e-05; cls_loss: 0.6910; all_acc4: 30.00; avg_acc4: 82.50
